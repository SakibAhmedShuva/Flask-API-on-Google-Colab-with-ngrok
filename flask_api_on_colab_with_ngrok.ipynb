{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6zCG3ZMQPFqI",
        "outputId": "4df89ba3-c703-4bc8-c9e5-47f3f44de474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Requests-2.32.4 accelerate-1.6.0 av-14.4.0 bitsandbytes-0.45.5 flask_cors-5.0.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 qwen-vl-utils-0.0.11 transformers-4.51.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7b1d070767214bd483c77567e2fc076f",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM7J_ADzOgrU",
        "outputId": "fee099e7-0910-4ce8-8ffe-1b12fbb50775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCXF-32lSZBW"
      },
      "outputs": [],
      "source": [
        "# In your Colab notebook, click the key icon (ðŸ”‘) on the left-hand side toolbar.\n",
        "# Click \"Add a new secret\".\n",
        "# For the Name, enter NGROK_AUTHTOKEN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "NGROK_AUTH_TOKEN=\"sdfgyhuioplkjhgfd\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "629fBys_XDC3"
      },
      "outputs": [],
      "source": [
        "# !pip install flask flask-cors torch transformers pillow requests numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prj0J8TbatA1"
      },
      "outputs": [],
      "source": [
        "# --- Core Setup (Move this BEFORE ngrok functions) ---\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Set up logging first\n",
        "LOGGING_LEVEL = os.getenv(\"LOGGING_LEVEL\", \"INFO\").upper()\n",
        "logging.basicConfig(level=getattr(logging, LOGGING_LEVEL, logging.INFO),\n",
        "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"flask_qwen_vl_app\")\n",
        "\n",
        "# --- Ngrok Setup ---\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "NGROK_STATIC_DOMAIN = \"firmly-chitah-haha.ngrok-free.app\"\n",
        "FLASK_PORT = int(os.getenv(\"FLASK_PORT\", 7860))  # Define FLASK_PORT before using it\n",
        "\n",
        "def install_ngrok():\n",
        "    \"\"\"Install pyngrok if not already installed\"\"\"\n",
        "    try:\n",
        "        import pyngrok\n",
        "        logger.info(\"pyngrok is already installed\")\n",
        "    except ImportError:\n",
        "        logger.info(\"Installing pyngrok...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyngrok\"])\n",
        "        import pyngrok\n",
        "        logger.info(\"pyngrok installed successfully\")\n",
        "\n",
        "def setup_ngrok():\n",
        "    \"\"\"Setup ngrok tunnel with static domain\"\"\"\n",
        "    try:\n",
        "        # Install ngrok if needed\n",
        "        install_ngrok()\n",
        "\n",
        "        # Set auth token if provided\n",
        "        if NGROK_AUTH_TOKEN:\n",
        "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "            logger.info(\"ngrok auth token set\")\n",
        "        else:\n",
        "            logger.warning(\"No NGROK_AUTH_TOKEN provided - ngrok may not work without authentication\")\n",
        "\n",
        "        # Kill any existing tunnels\n",
        "        ngrok.kill()\n",
        "\n",
        "        # Create new tunnel with static domain\n",
        "        if NGROK_STATIC_DOMAIN:\n",
        "            # Use static domain\n",
        "            public_tunnel = ngrok.connect(FLASK_PORT, hostname=NGROK_STATIC_DOMAIN)\n",
        "            public_url = f\"https://{NGROK_STATIC_DOMAIN}\"\n",
        "            logger.info(f\"ðŸŒ Using static domain: {NGROK_STATIC_DOMAIN}\")\n",
        "        else:\n",
        "            # Fallback to random domain\n",
        "            public_tunnel = ngrok.connect(FLASK_PORT)\n",
        "            public_url = public_tunnel.public_url\n",
        "            logger.info(f\"ðŸŒ Using random domain\")\n",
        "\n",
        "        logger.info(f\"ðŸ”— Your Flask app is now accessible at: {public_url}\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to setup ngrok: {e}\")\n",
        "        logger.info(\"App will run locally only\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFddOWXHcVzQ",
        "outputId": "87cd2716-2005-4089-8c19-1c6abe649e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:7860\n",
            " * Running on http://172.28.0.12:7860\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [18/Jun/2025 19:43:05] \"GET / HTTP/1.1\" 200 -\n",
            "WARNING:pyngrok.process.ngrok:t=2025-06-18T19:43:05+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-06-18T19:43:12+0000 lvl=warn msg=\"Stopping forwarder\" name=http-7860-c4af71ad-6b5e-452f-bb7b-ab95193d14f8 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "# --- Flask API Imports ---\n",
        "from flask import Flask, request, jsonify, Response, stream_with_context\n",
        "from flask_cors import CORS\n",
        "\n",
        "# --- Original Core Logic Imports (Now used by Flask) ---\n",
        "import logging\n",
        "import re\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import os\n",
        "import uuid\n",
        "import json\n",
        "import time\n",
        "from threading import Thread, Lock, Semaphore\n",
        "from collections import deque\n",
        "import csv\n",
        "import datetime\n",
        "import torch\n",
        "from transformers import AutoProcessor, TextIteratorStreamer, Qwen2_5_VLForConditionalGeneration\n",
        "import numpy # For qwen_model_handler\n",
        "\n",
        "# --- Global Configuration ---\n",
        "LOGGING_LEVEL = os.getenv(\"LOGGING_LEVEL\", \"INFO\").upper()\n",
        "FLASK_PORT = int(os.getenv(\"FLASK_PORT\", 7860))\n",
        "FLASK_DEBUG = os.getenv(\"FLASK_DEBUG\", \"True\").lower() == 'true'\n",
        "PRELOAD_DEFAULT_MODEL = os.getenv(\"PRELOAD_DEFAULT_MODEL\", \"false\").lower() == \"true\"\n",
        "MAX_CONCURRENT_REQUESTS = int(os.getenv(\"MAX_CONCURRENT_REQUESTS\", 3))\n",
        "\n",
        "# --- Model Configurations ---\n",
        "MODEL_CONFIGS = [\n",
        "    { \"id\": \"unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit\", \"display_name\": \"Qwen2.5-VL 3B (Instruct, 4-bit Unsloth)\" },\n",
        "    { \"id\": \"unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\", \"display_name\": \"Qwen2.5-VL 7B (Instruct, 4-bit Unsloth)\" },\n",
        "    { \"id\": \"unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit\", \"display_name\": \"Qwen2.5-VL 32B (Instruct, 4-bit Unsloth)\" }\n",
        "]\n",
        "DEFAULT_MODEL_CONFIG = MODEL_CONFIGS[1]\n",
        "DEFAULT_MODEL_ID = DEFAULT_MODEL_CONFIG[\"id\"]\n",
        "DEFAULT_MODEL_DISPLAY_NAME = DEFAULT_MODEL_CONFIG[\"display_name\"]\n",
        "MODEL_DISPLAY_NAME_TO_ID_MAP = {config[\"display_name\"]: config[\"id\"] for config in MODEL_CONFIGS}\n",
        "\n",
        "DEFAULT_TEMPERATURE = float(os.getenv(\"DEFAULT_TEMPERATURE\", 0.1))\n",
        "\n",
        "# --- Performance & Loading Defaults ---\n",
        "DEFAULT_LOAD_PARAMS = {\n",
        "    \"load_in_4bit\": True,\n",
        "    \"bnb_4bit_quant_type\": os.getenv(\"DEFAULT_BNB_4BIT_QUANT_TYPE\", \"nf4\"),\n",
        "    \"bnb_4bit_use_double_quant\": os.getenv(\"DEFAULT_BNB_4BIT_USE_DOUBLE_QUANT\", \"true\").lower() == 'true',\n",
        "    \"bnb_4bit_compute_dtype_str\": os.getenv(\"DEFAULT_BNB_4BIT_COMPUTE_DTYPE\", \"auto\"),\n",
        "    \"llm_int8_enable_fp32_cpu_offload\": os.getenv(\"DEFAULT_LLM_INT8_ENABLE_FP32_CPU_OFFLOAD\", \"false\").lower() == 'true',\n",
        "}\n",
        "\n",
        "# --- Core Setup ---\n",
        "logging.basicConfig(level=getattr(logging, LOGGING_LEVEL, logging.INFO), format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(\"flask_qwen_vl_app\")\n",
        "LOGS_DIRECTORY = os.path.join(os.getcwd(), 'logs')\n",
        "TEMP_IMAGE_DIRECTORY = os.path.join(LOGS_DIRECTORY, 'temp_images')\n",
        "os.makedirs(TEMP_IMAGE_DIRECTORY, exist_ok=True)\n",
        "USAGE_LOG_FILE = os.path.join(LOGS_DIRECTORY, 'usage_log_flask.csv')\n",
        "USAGE_LOG_HEADERS = [\"timestamp\", \"user_identifier\", \"request_uuid\", \"model_id\", \"temperature\", \"system_prompt_key_used\", \"prompt_text\", \"image_provided\", \"image_details\", \"ai_response_length\", \"ai_response_preview\", \"ttft_ms\", \"generation_time_ms\", \"total_stream_handler_time_ms\", \"error_message\", \"load_params_used\"]\n",
        "usage_log_lock = Lock()\n",
        "loaded_models_cache = {}\n",
        "\n",
        "# --- Concurrency & Queue Management ---\n",
        "logger.info(f\"Setting max concurrent requests to: {MAX_CONCURRENT_REQUESTS}\")\n",
        "request_semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "queue_lock = Lock()\n",
        "request_queue = deque()  # Stores request_uuids of waiting requests\n",
        "active_requests = {}     # Maps request_uuid to its start time for active requests\n",
        "\n",
        "\n",
        "def init_usage_log_csv():\n",
        "    with usage_log_lock:\n",
        "        if not os.path.exists(USAGE_LOG_FILE):\n",
        "            with open(USAGE_LOG_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "                csv.writer(f).writerow(USAGE_LOG_HEADERS)\n",
        "\n",
        "def log_usage_data(**kwargs):\n",
        "    with usage_log_lock:\n",
        "        row = [kwargs.get(h, \"\") for h in USAGE_LOG_HEADERS]\n",
        "        with open(USAGE_LOG_FILE, mode='a', newline='', encoding='utf-8') as f:\n",
        "            csv.writer(f).writerow(row)\n",
        "\n",
        "def _load_image_from_url(url):\n",
        "    try:\n",
        "        if url.startswith('http'):\n",
        "            response = requests.get(url, stream=True, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "        elif os.path.exists(url):\n",
        "            img = Image.open(url)\n",
        "        else: return None\n",
        "        return img.convert('RGB') if img.mode != 'RGB' else img\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading image {url[:60]}...: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_pil_images_from_messages(messages):\n",
        "    pil_images = []\n",
        "    if not messages: return pil_images\n",
        "    for msg in messages:\n",
        "        if msg.get(\"role\") == \"user\" and isinstance(msg.get(\"content\"), list):\n",
        "            for item in msg[\"content\"]:\n",
        "                if item.get(\"type\") == \"image_url\":\n",
        "                    url_data = item.get(\"image_url\", {})\n",
        "                    url = url_data.get(\"url\")\n",
        "                    if url and (img := _load_image_from_url(url)):\n",
        "                        pil_images.append(img)\n",
        "    return pil_images\n",
        "\n",
        "DEFAULT_SYSTEM_PROMPTS = {\n",
        "    \"ocr_digit_only\": \"You are an expert Optical Character Recognition (OCR) assistant. Your sole task is to meticulously extract ONLY THE DIGITS (0-9) visible in the provided image. Present these digits clearly, for example, separated by spaces or newlines. If no digits are found, explicitly state 'No digits found'. Do not provide any other text, explanation, or commentary. If no image is provided, state 'Please provide an image for digit extraction.'\",\n",
        "    \"ocr_general\": \"You are an expert OCR assistant. Your primary task is to meticulously extract ALL text, numbers, and symbols visible in any provided image or described scene. Transcribe the text exactly as it appears. Only output the extracted text. If no image is clearly referenced or uploaded, state that you need an image or image URL to perform OCR.\",\n",
        "    \"ocr_receipt\": \"You are an expert OCR assistant specializing in receipts. Extract all items, quantities, and prices. Also identify the store name, date, and total amount. Present the information in a structured format if possible.\",\n",
        "    \"chat_general_helper\": \"You are a helpful AI assistant. Analyze the provided image and respond to the user's query.\"\n",
        "}\n",
        "PRIMARY_DEFAULT_SYSTEM_PROMPT_KEY = \"ocr_digit_only\"\n",
        "\n",
        "def get_dtype_from_string(dtype_str: str):\n",
        "    if dtype_str == \"bfloat16\": return torch.bfloat16\n",
        "    if dtype_str == \"float16\": return torch.float16\n",
        "    if dtype_str == \"float32\": return torch.float32\n",
        "    return \"auto\"\n",
        "\n",
        "def get_model_and_processor(model_id: str, load_in_4bit: bool, bnb_4bit_quant_type: str, bnb_4bit_use_double_quant: bool, bnb_4bit_compute_dtype_str: str, llm_int8_enable_fp32_cpu_offload: bool):\n",
        "    param_tuple = (model_id, f\"4bit-{load_in_4bit}\", f\"quant-{bnb_4bit_quant_type}\", f\"doubleq-{bnb_4bit_use_double_quant}\", f\"compute-{bnb_4bit_compute_dtype_str}\", f\"offload-{llm_int8_enable_fp32_cpu_offload}\")\n",
        "    cache_key = \"_\".join(param_tuple)\n",
        "    if cache_key in loaded_models_cache: return loaded_models_cache[cache_key]\n",
        "\n",
        "    logger.info(f\"Initiating load for model '{model_id}'. Cache key: {cache_key}\")\n",
        "    model_kwargs = {\"trust_remote_code\": True}\n",
        "    if torch.cuda.is_available():\n",
        "        model_kwargs[\"device_map\"] = \"auto\"\n",
        "        actual_compute_dtype = get_dtype_from_string(bnb_4bit_compute_dtype_str)\n",
        "        if actual_compute_dtype == \"auto\":\n",
        "            actual_compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "        model_kwargs.update({\n",
        "            \"load_in_4bit\": load_in_4bit,\n",
        "            \"bnb_4bit_quant_type\": bnb_4bit_quant_type,\n",
        "            \"bnb_4bit_use_double_quant\": bnb_4bit_use_double_quant,\n",
        "            \"bnb_4bit_compute_dtype\": actual_compute_dtype\n",
        "        })\n",
        "        if llm_int8_enable_fp32_cpu_offload:\n",
        "            model_kwargs[\"llm_int8_enable_fp32_cpu_offload\"] = True\n",
        "    else:\n",
        "        logger.warning(f\"CUDA NOT available. Model '{model_id}' will be loaded on CPU.\")\n",
        "        model_kwargs.update({\"device_map\": \"cpu\", \"torch_dtype\": torch.float32})\n",
        "\n",
        "    try:\n",
        "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id, **model_kwargs)\n",
        "        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "        loaded_models_cache[cache_key] = (model, processor)\n",
        "        logger.info(f\"Successfully loaded model and processor for: {model_id}\")\n",
        "        return model, processor\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model '{model_id}': {e}\", exc_info=True)\n",
        "        if cache_key in loaded_models_cache: del loaded_models_cache[cache_key]\n",
        "        raise e\n",
        "\n",
        "def generate_chat_response_blocking(model_id_param: str, messages_for_model, temperature, **load_params):\n",
        "    model, processor = get_model_and_processor(model_id_param, **load_params)\n",
        "    pil_images = extract_pil_images_from_messages(messages_for_model)\n",
        "    text_prompt = processor.apply_chat_template(messages_for_model, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text_prompt], images=pil_images or None, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generation_kwargs = dict(inputs, max_new_tokens=2048, temperature=max(temperature, 0.01))\n",
        "    outputs = model.generate(**generation_kwargs)\n",
        "\n",
        "    # Decode only the newly generated tokens, excluding the prompt\n",
        "    generated_tokens = outputs[:, inputs.input_ids.shape[-1]:]\n",
        "    response_text = processor.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def generate_chat_response_stream(model_id_param: str, messages_for_model, temperature, **load_params):\n",
        "    model, processor = get_model_and_processor(model_id_param, **load_params)\n",
        "    pil_images = extract_pil_images_from_messages(messages_for_model)\n",
        "    text_prompt = processor.apply_chat_template(messages_for_model, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text_prompt], images=pil_images or None, return_tensors=\"pt\").to(model.device)\n",
        "    streamer = TextIteratorStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=2048, temperature=max(temperature, 0.01))\n",
        "    Thread(target=model.generate, kwargs=generation_kwargs).start()\n",
        "\n",
        "    buffer = \"\"\n",
        "    for chunk in streamer:\n",
        "        if chunk: buffer += chunk\n",
        "        if ' ' in buffer or '\\n' in buffer or len(buffer) > 5:\n",
        "            yield buffer\n",
        "            buffer = \"\"\n",
        "    if buffer: yield buffer\n",
        "\n",
        "def prepare_qwen_messages_for_model(current_message_dict, system_prompt_text, request_uuid, system_prompt_key):\n",
        "    qwen_messages = [{\"role\": \"system\", \"content\": system_prompt_text}]\n",
        "    logged_image_path = None\n",
        "\n",
        "    current_user_prompt_text = current_message_dict.get(\"text\", \"\")\n",
        "    current_qwen_user_content_parts = []\n",
        "    image_part_added = False\n",
        "\n",
        "    if current_message_dict.get(\"files\"):\n",
        "        temp_image_path = current_message_dict[\"files\"][0]\n",
        "        current_qwen_user_content_parts.append({\"type\": \"image_url\", \"image_url\": {\"url\": temp_image_path}})\n",
        "        image_part_added = True\n",
        "        if pil_image_to_log := _load_image_from_url(temp_image_path):\n",
        "            logged_image_path = os.path.join(LOGS_DIRECTORY, f\"log_img_{request_uuid}.png\")\n",
        "            pil_image_to_log.save(logged_image_path)\n",
        "\n",
        "    if current_user_prompt_text.strip():\n",
        "        current_qwen_user_content_parts.append({\"type\": \"text\", \"text\": current_user_prompt_text.strip()})\n",
        "    elif image_part_added:\n",
        "        default_text = \"ocr digit\" if system_prompt_key == \"ocr_digit_only\" else \"Describe the image.\"\n",
        "        current_qwen_user_content_parts.append({\"type\": \"text\", \"text\": default_text})\n",
        "\n",
        "    if current_qwen_user_content_parts:\n",
        "        qwen_messages.append({\"role\": \"user\", \"content\": current_qwen_user_content_parts})\n",
        "\n",
        "    return qwen_messages, logged_image_path\n",
        "\n",
        "def process_chat_request_blocking(request_uuid, **args):\n",
        "    actual_model_id = MODEL_DISPLAY_NAME_TO_ID_MAP.get(args['model_display_name'], DEFAULT_MODEL_ID)\n",
        "    system_prompt_text = DEFAULT_SYSTEM_PROMPTS.get(args['system_prompt_key'], DEFAULT_SYSTEM_PROMPTS[PRIMARY_DEFAULT_SYSTEM_PROMPT_KEY])\n",
        "\n",
        "    if not args['current_message'].get(\"text\", \"\").strip() and not args['current_message'].get(\"files\", []):\n",
        "        return {\"error\": \"Please provide some input or an image.\"}, 400\n",
        "\n",
        "    messages_for_model, logged_image_path = prepare_qwen_messages_for_model(args['current_message'], system_prompt_text, request_uuid, args['system_prompt_key'])\n",
        "    if len(messages_for_model) < 2:\n",
        "        return {\"error\": \"Could not form a valid message to send.\"}, 400\n",
        "\n",
        "    load_params = {k: v for k, v in args.items() if k in DEFAULT_LOAD_PARAMS}\n",
        "    user_prompt_text_for_log = next((item[\"text\"] for item in messages_for_model[-1][\"content\"] if item[\"type\"] == \"text\"), \"\")\n",
        "    log_payload = {\"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(), \"user_identifier\": \"api_user\", \"request_uuid\": request_uuid, \"model_id\": actual_model_id, \"temperature\": args['temperature'], \"system_prompt_key_used\": args['system_prompt_key'], \"prompt_text\": user_prompt_text_for_log, \"image_provided\": bool(logged_image_path), \"image_details\": logged_image_path or \"none\", \"load_params_used\": json.dumps(load_params)}\n",
        "\n",
        "    full_ai_response = \"\"\n",
        "    start_time = time.monotonic()\n",
        "    try:\n",
        "        full_ai_response = generate_chat_response_blocking(\n",
        "            model_id_param=actual_model_id,\n",
        "            messages_for_model=messages_for_model,\n",
        "            temperature=args['temperature'],\n",
        "            **load_params\n",
        "        )\n",
        "        end_time = time.monotonic()\n",
        "        total_time_ms = round((end_time - start_time) * 1000)\n",
        "\n",
        "        log_payload.update({\"ai_response_length\": len(full_ai_response), \"ai_response_preview\": full_ai_response[:200], \"ttft_ms\": total_time_ms, \"generation_time_ms\": total_time_ms, \"total_stream_handler_time_ms\": total_time_ms, \"error_message\": \"\"})\n",
        "\n",
        "        response_data = {\"content\": full_ai_response, \"request_uuid\": request_uuid, \"model_id\": actual_model_id}\n",
        "        return response_data, 200\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during blocking model generation for {request_uuid}: {e}\", exc_info=True)\n",
        "        total_time_ms = round((time.monotonic() - start_time) * 1000)\n",
        "        log_payload.update({\"error_message\": str(e)[:200], \"total_stream_handler_time_ms\": total_time_ms})\n",
        "        return {\"error\": f\"Error during model generation: {str(e)[:200]}\", \"request_uuid\": request_uuid}, 500\n",
        "    finally:\n",
        "        log_usage_data(**log_payload)\n",
        "\n",
        "\n",
        "def process_chat_request_stream(request_uuid, **args):\n",
        "    actual_model_id = MODEL_DISPLAY_NAME_TO_ID_MAP.get(args['model_display_name'], DEFAULT_MODEL_ID)\n",
        "    system_prompt_text = DEFAULT_SYSTEM_PROMPTS.get(args['system_prompt_key'], DEFAULT_SYSTEM_PROMPTS[PRIMARY_DEFAULT_SYSTEM_PROMPT_KEY])\n",
        "\n",
        "    if not args['current_message'].get(\"text\", \"\").strip() and not args['current_message'].get(\"files\", []):\n",
        "        yield f\"data: {json.dumps({'error': 'Please provide some input or an image.'})}\\n\\n\"\n",
        "        return\n",
        "\n",
        "    messages_for_model, logged_image_path = prepare_qwen_messages_for_model(args['current_message'], system_prompt_text, request_uuid, args['system_prompt_key'])\n",
        "    if len(messages_for_model) < 2:\n",
        "        yield f\"data: {json.dumps({'error': 'Could not form a valid message to send.'})}\\n\\n\"\n",
        "        return\n",
        "\n",
        "    load_params = {k: v for k, v in args.items() if k in DEFAULT_LOAD_PARAMS}\n",
        "    user_prompt_text_for_log = next((item[\"text\"] for item in messages_for_model[-1][\"content\"] if item[\"type\"] == \"text\"), \"\")\n",
        "    log_payload = {\"timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat(), \"user_identifier\": \"api_user\", \"request_uuid\": request_uuid, \"model_id\": actual_model_id, \"temperature\": args['temperature'], \"system_prompt_key_used\": args['system_prompt_key'], \"prompt_text\": user_prompt_text_for_log, \"image_provided\": bool(logged_image_path), \"image_details\": logged_image_path or \"none\", \"load_params_used\": json.dumps(load_params)}\n",
        "\n",
        "    full_ai_response = \"\"\n",
        "    stream_start_time = time.monotonic()\n",
        "    first_chunk_time = None\n",
        "    try:\n",
        "        stream_gen = generate_chat_response_stream(\n",
        "            model_id_param=actual_model_id,\n",
        "            messages_for_model=messages_for_model,\n",
        "            temperature=args['temperature'],\n",
        "            **load_params\n",
        "        )\n",
        "        for chunk in stream_gen:\n",
        "            if first_chunk_time is None: first_chunk_time = time.monotonic()\n",
        "            if chunk:\n",
        "                full_ai_response += chunk\n",
        "                yield f\"data: {json.dumps({'content': full_ai_response})}\\n\\n\" # Yield cumulative response\n",
        "\n",
        "        total_time_ms = round((time.monotonic() - stream_start_time) * 1000)\n",
        "        ttft_ms = round((first_chunk_time - stream_start_time) * 1000) if first_chunk_time else total_time_ms\n",
        "        gen_time_ms = round((time.monotonic() - first_chunk_time) * 1000) if first_chunk_time else 0\n",
        "        log_payload.update({\"ai_response_length\": len(full_ai_response), \"ai_response_preview\": full_ai_response[:200], \"ttft_ms\": ttft_ms, \"generation_time_ms\": gen_time_ms, \"total_stream_handler_time_ms\": total_time_ms, \"error_message\": \"\"})\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during model generation for {request_uuid}: {e}\", exc_info=True)\n",
        "        total_time_ms = round((time.monotonic() - stream_start_time) * 1000)\n",
        "        yield f\"data: {json.dumps({'error': f'ERROR: {str(e)[:200]}'})}\\n\\n\"\n",
        "        log_payload.update({\"error_message\": str(e)[:200], \"total_stream_handler_time_ms\": total_time_ms})\n",
        "    finally:\n",
        "        log_usage_data(**log_payload)\n",
        "        # Signal end of stream\n",
        "        yield f\"data: {json.dumps({'event': 'done'})}\\n\\n\"\n",
        "\n",
        "def preload_models_on_startup():\n",
        "    if PRELOAD_DEFAULT_MODEL:\n",
        "        logger.info(f\"Attempting to preload default model: {DEFAULT_MODEL_ID}...\")\n",
        "        try:\n",
        "            get_model_and_processor(DEFAULT_MODEL_ID, **DEFAULT_LOAD_PARAMS)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to preload default model {DEFAULT_MODEL_ID}: {e}\", exc_info=True)\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "def health_check(): return jsonify({\"status\": \"ok\", \"message\": \"Qwen-VL API is running.\"})\n",
        "\n",
        "@app.route('/api/queue_status', methods=['GET'])\n",
        "def queue_status():\n",
        "    request_uuid = request.args.get('request_uuid')\n",
        "    with queue_lock:\n",
        "        # Create copies to avoid issues with modifying the collections while iterating\n",
        "        active_list = list(active_requests.keys())\n",
        "        waiting_list = list(request_queue)\n",
        "\n",
        "    status = {\n",
        "        \"max_concurrent_requests\": MAX_CONCURRENT_REQUESTS,\n",
        "        \"active_requests_count\": len(active_list),\n",
        "        \"queued_requests_count\": len(waiting_list),\n",
        "        \"active_requests\": active_list,\n",
        "        \"queued_requests\": waiting_list,\n",
        "    }\n",
        "\n",
        "    if request_uuid:\n",
        "        if request_uuid in active_list:\n",
        "            status['your_status'] = 'processing'\n",
        "            status['your_queue_position'] = 0\n",
        "        elif request_uuid in waiting_list:\n",
        "            try:\n",
        "                # +1 for 1-based indexing\n",
        "                position = waiting_list.index(request_uuid) + 1\n",
        "                status['your_status'] = 'queued'\n",
        "                status['your_queue_position'] = position\n",
        "            except ValueError:\n",
        "                # Should not happen due to the lock, but as a safeguard\n",
        "                status['your_status'] = 'unknown'\n",
        "                status['your_queue_position'] = -1\n",
        "        else:\n",
        "            status['your_status'] = 'completed_or_unknown'\n",
        "            status['your_queue_position'] = -1\n",
        "\n",
        "    return jsonify(status)\n",
        "\n",
        "@app.route('/api/chat', methods=['POST'])\n",
        "def chat_endpoint():\n",
        "    temp_image_path = None\n",
        "    request_uuid = str(uuid.uuid4())\n",
        "\n",
        "    try:\n",
        "        stream = request.form.get('stream', 'true').lower() == 'true'\n",
        "\n",
        "        image_file = request.files.get('image_file')\n",
        "        temp_files_list = []\n",
        "        if image_file and image_file.filename:\n",
        "            temp_image_path = os.path.join(TEMP_IMAGE_DIRECTORY, f\"api_img_{uuid.uuid4()}_{image_file.filename}\")\n",
        "            image_file.save(temp_image_path)\n",
        "            temp_files_list.append(temp_image_path)\n",
        "\n",
        "        def get_form_val(key, default, converter):\n",
        "            val = request.form.get(key)\n",
        "            return converter(val) if val is not None else default\n",
        "\n",
        "        load_params = {\n",
        "            \"load_in_4bit\": get_form_val(\"load_in_4bit\", DEFAULT_LOAD_PARAMS[\"load_in_4bit\"], lambda v: v.lower() == 'true'),\n",
        "            \"bnb_4bit_quant_type\": request.form.get(\"bnb_4bit_quant_type\", DEFAULT_LOAD_PARAMS[\"bnb_4bit_quant_type\"]),\n",
        "            \"bnb_4bit_use_double_quant\": get_form_val(\"bnb_4bit_use_double_quant\", DEFAULT_LOAD_PARAMS[\"bnb_4bit_use_double_quant\"], lambda v: v.lower() == 'true'),\n",
        "            \"bnb_4bit_compute_dtype_str\": request.form.get(\"bnb_4bit_compute_dtype_str\", DEFAULT_LOAD_PARAMS[\"bnb_4bit_compute_dtype_str\"]),\n",
        "            \"llm_int8_enable_fp32_cpu_offload\": get_form_val(\"llm_int8_enable_fp32_cpu_offload\", DEFAULT_LOAD_PARAMS[\"llm_int8_enable_fp32_cpu_offload\"], lambda v: v.lower() == 'true')\n",
        "        }\n",
        "\n",
        "        args = {\n",
        "            \"current_message\": {\"text\": request.form.get(\"message\", \"\"), \"files\": temp_files_list},\n",
        "            \"model_display_name\": request.form.get(\"model_display_name\", DEFAULT_MODEL_DISPLAY_NAME),\n",
        "            \"system_prompt_key\": request.form.get(\"system_prompt_key\", PRIMARY_DEFAULT_SYSTEM_PROMPT_KEY),\n",
        "            \"temperature\": get_form_val(\"temperature\", DEFAULT_TEMPERATURE, float),\n",
        "            **load_params\n",
        "        }\n",
        "\n",
        "        if stream:\n",
        "            def generate_stream_response():\n",
        "                with queue_lock:\n",
        "                    request_queue.append(request_uuid)\n",
        "                    position = len(request_queue)\n",
        "                    active_count = len(active_requests)\n",
        "\n",
        "                yield f\"data: {json.dumps({'event': 'queued', 'request_uuid': request_uuid, 'queue_position': position, 'active_requests': active_count})}\\n\\n\"\n",
        "\n",
        "                request_semaphore.acquire()\n",
        "                try:\n",
        "                    with queue_lock:\n",
        "                        if request_uuid in request_queue:\n",
        "                            request_queue.remove(request_uuid)\n",
        "                        active_requests[request_uuid] = time.monotonic()\n",
        "\n",
        "                    yield f\"data: {json.dumps({'event': 'processing_started', 'request_uuid': request_uuid})}\\n\\n\"\n",
        "                    yield from process_chat_request_stream(request_uuid, **args)\n",
        "                finally:\n",
        "                    with queue_lock:\n",
        "                        active_requests.pop(request_uuid, None)\n",
        "                    request_semaphore.release()\n",
        "\n",
        "            return Response(stream_with_context(generate_stream_response()), mimetype='text/event-stream')\n",
        "        else: # Blocking request\n",
        "            with queue_lock:\n",
        "                request_queue.append(request_uuid)\n",
        "\n",
        "            request_semaphore.acquire()\n",
        "            try:\n",
        "                with queue_lock:\n",
        "                    if request_uuid in request_queue:\n",
        "                        request_queue.remove(request_uuid)\n",
        "                    active_requests[request_uuid] = time.monotonic()\n",
        "\n",
        "                response_data, status_code = process_chat_request_blocking(request_uuid, **args)\n",
        "                return jsonify(response_data), status_code\n",
        "            finally:\n",
        "                with queue_lock:\n",
        "                    active_requests.pop(request_uuid, None)\n",
        "                request_semaphore.release()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled error in chat endpoint for {request_uuid}: {e}\", exc_info=True)\n",
        "        with queue_lock:\n",
        "            if request_uuid in request_queue:\n",
        "                request_queue.remove(request_uuid)\n",
        "        return jsonify({\"error\": \"An unexpected server error occurred.\", \"request_uuid\": request_uuid}), 500\n",
        "    finally:\n",
        "        if temp_image_path and os.path.exists(temp_image_path):\n",
        "            os.remove(temp_image_path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init_usage_log_csv()\n",
        "\n",
        "    # Setup ngrok (optional)\n",
        "    public_url = setup_ngrok()\n",
        "\n",
        "    # Preload models if requested\n",
        "    preload_models_on_startup()\n",
        "\n",
        "    logger.info(\"Starting Qwen-VL Flask API Server...\")\n",
        "    logger.info(f\"Server will run on http://0.0.0.0:{FLASK_PORT}\")\n",
        "    if public_url:\n",
        "        logger.info(f\"Public URL: {public_url}\")\n",
        "\n",
        "    app.run(host=\"0.0.0.0\", port=FLASK_PORT, debug=FLASK_DEBUG, use_reloader=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
